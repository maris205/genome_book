{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69109b9d-c5f3-4de2-a748-b2cb74f880f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# 打印环境变量以确认设置成功\n",
    "print(os.environ.get('HF_ENDPOINT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960559d5-af43-4db8-ab35-6cf74d47461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maris/miniconda3/envs/dnagpt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling,AutoTokenizer\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d955df1a-04b2-45b0-ac69-48f49bfdf71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b808b243-4d24-4300-9209-ffa59c4a81c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(100000, 768)\n",
       "    (wpe): Embedding(256, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=100000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path=\"dnagpt/gene_eng_gpt2_v1_ft\"\n",
    "# 加载模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.to(device)  # 将模型加载到设备上\n",
    "model.eval()  # 设置为评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f3e8f9-6589-4d0c-8d8d-2dfd4da007e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_sliding_window(text, max_length=256, stride=64):\n",
    "    \"\"\"\n",
    "    使用滑动窗口将超长文本切分为多个段，每段长度不超过 max_length。\n",
    "    \"\"\"\n",
    "    # 分词\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # 滑动窗口分段\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokenized_text):\n",
    "        end = min(start + max_length, len(tokenized_text))\n",
    "        chunks.append(tokenizer.decode(tokenized_text[start:end], skip_special_tokens=True))\n",
    "        start += max_length - stride  # 移动窗口，保留 overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def segment_text(text, top_k=15, max_length=256, stride=64):\n",
    "    \"\"\"\n",
    "    使用微调后的 GPT-2 模型对输入长文本进行段落分割，动态调整阈值。\n",
    "    \"\"\"\n",
    "    # 分段文本\n",
    "    chunks = split_text_sliding_window(text, max_length, stride)\n",
    "\n",
    "    # 对每段文本进行推理\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}  # 将所有张量移到同一设备\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)  # 确保模型和输入都在同一设备\n",
    "            logits = outputs.logits  # [batch_size, seq_length, vocab_size]\n",
    "\n",
    "        # 找到 <p_end> 的 token ID\n",
    "        p_end_id = tokenizer.convert_tokens_to_ids(\".\")\n",
    "\n",
    "        # 计算 <p_end> 的概率分布\n",
    "        probabilities = torch.softmax(logits, dim=-1)  # 转为概率分布\n",
    "        p_end_probs = probabilities[0, :, p_end_id].to(device)  # 确保在相同设备上\n",
    "\n",
    "        # 动态计算阈值：取前 top_k 概率的平均值\n",
    "        sorted_probs, _ = torch.sort(p_end_probs, descending=True)\n",
    "        threshold = sorted_probs[:top_k].mean().item()\n",
    "\n",
    "        # 根据动态阈值插入段落标记\n",
    "        tokens = inputs[\"input_ids\"][0].tolist()\n",
    "        segmented_tokens = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            segmented_tokens.append(token)\n",
    "            if p_end_probs[i].item() > threshold:\n",
    "                segmented_tokens.append(p_end_id)\n",
    "\n",
    "        # 解码为文本\n",
    "        segmented_text = tokenizer.decode(segmented_tokens, skip_special_tokens=False)\n",
    "        results.append(segmented_text)\n",
    "\n",
    "    # 合并结果\n",
    "    return \" \".join(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faab54cb-a198-4614-9d4b-417bc6c57bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTTACTACAGTGGACATCAAGGGCACATTCTTGCTGTGGCCATCAAGAGACTGTATAAATTCTATGACTTGTAGTTGTCCCACTTAAGAAACAAAGAAGCTGTGCATTTCTTTACTGGTCTAGAGCTGCTCTAGGGCATTTTCTCTACAGCAATTCTAGGTTTCCCCACCTTGTGAGTTTAGCTTTTTCTATATTCAAAGAAAAGTCCTCAGCCAGAGATTCTCAGGAGCTTATAGAACAATCCAAACTCTTGGGAATATTAAGTGGAGAGGGGTACGTGCAAGACACCAACAGCACTAGAAACAG\n",
      "ATTTAC TACAGTGG ACATC AAGGGC ACATTC TTGC TGTGGCC ATCAAG AGACTG TATAAATTC TATG ACTTG TAGTTG TCCC ACTT AAGAAACAA AGAAGC TGTGC ATTTCTT TACTGG TCTAG AGCTGC TCTAGGGC ATTTTC TCTAC AGCAA TTCTAGG TTTCCCC ACCTTG TGAG TTTAGC TTTT TCTATA TTCAAAG AAAAGTCC TCAGCC AGAGATTC TCAGGAGC TTATAG AACAA TCCAAAC TCTT GGGAA TATT AAGTGG AGAGGGG TACG TGCAAG . ACACC AACAGC ACTAGAA . ACAG\n"
     ]
    }
   ],
   "source": [
    "# 示例输入长文本\n",
    "input_text = \"Multilingual transfer ability, which reflects how well models fine-tuned on one source language can be applied to other languages, has been well studied in multilingual pre-trained models. However, the existence of such capability transfer between natural language and gene sequences/languages remains under explored.This study addresses this gap by drawing inspiration from the sentence-pair classification task used for evaluating sentence similarity in natural language. We constructed two analogous tasks: DNA-pair classification(DNA sequence similarity) and DNA-protein-pair classification(gene coding determination). These tasks were designed to validate the transferability of capabilities from natural language to gene sequences. Even a small-scale pre-trained model like GPT-2-small, which was pre-trained on English, achieved an accuracy of 78% on the DNA-pair classification task after being fine-tuned on English sentence-pair classification data(XTREME PAWS-X).  While training a BERT model on multilingual text, the precision reached 89%. On the more complex DNA-protein-pair classification task, however, the model's output was barely distinguishable from random output.Experimental validation has confirmed that the transfer of capabilities from natural language to biological language is unequivocally present. Building on this foundation, we have also investigated the impact of model parameter scale and pre-training on this capability transfer. We provide recommendations for facilitating the transfer of capabilities from natural language to genetic language,as well as new approaches for conducting biological research based on this capability.This study offers an intriguing new perspective on exploring the relationship between natural language and genetic language.\"\n",
    "input_text = \"ATTTACTACAGTGGACATCAAGGGCACATTCTTGCTGTGGCCATCAAGAGACTGTATAAATTCTATGACTTGTAGTTGTCCCACTTAAGAAACAAAGAAGCTGTGCATTTCTTTACTGGTCTAGAGCTGCTCTAGGGCATTTTCTCTACAGCAATTCTAGGTTTCCCCACCTTGTGAGTTTAGCTTTTTCTATATTCAAAGAAAAGTCCTCAGCCAGAGATTCTCAGGAGCTTATAGAACAATCCAAACTCTTGGGAATATTAAGTGGAGAGGGGTACGTGCAAGACACCAACAGCACTAGAAACAG\"\n",
    "input_text_noseq = input_text.replace(\".\",\"\")\n",
    "\n",
    "# 调用分段函数\n",
    "segmented_output = segment_text(input_text_noseq)\n",
    "print(input_text)\n",
    "print(segmented_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76704a2f-0845-41a8-8887-1f571b92768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#读取第1层的数据\n",
    "json_str = open(\"p1_cluster_data_with_title.json\",\"r\").read()\n",
    "p1_cluster_data_with_title = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267facef-ec29-4226-aae3-944c28d703d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_cluster_data_with_title_sentence = []\n",
    "for item in p1_cluster_data_with_title:\n",
    "    para_list = item[\"para_list\"]\n",
    "    #对每个段落进行分词分句\n",
    "    para_sentence_list = []\n",
    "    for para in para_list:\n",
    "        segmented_output = segment_text(para)\n",
    "        para_sentence_list.append(segmented_output)\n",
    "\n",
    "    item[\"para_sentence_list\"] = para_sentence_list\n",
    "\n",
    "    p1_cluster_data_with_title_sentence.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34467f-790b-4698-9b9c-6fd821bd7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 保存到 JSON 文件\n",
    "with open(\"p1_cluster_data_with_title_sentence.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(p1_cluster_data_with_title_sentence, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"数据已保存到 p1_cluster_data_with_title_sentence.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e796d98-c04e-4dd3-a23d-3dec2c6e613d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
