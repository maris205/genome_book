{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69109b9d-c5f3-4de2-a748-b2cb74f880f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29074d5-5b8d-408a-b24a-da794563be49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:39:19.934787: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-20 12:39:19.947842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-20 12:39:19.963086: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-20 12:39:19.967727: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-20 12:39:19.980058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-20 12:39:20.778316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf15f676-f1b7-4450-a197-994513da4cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847518c9812449a391909fcf07182f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688b811d6d8c466b96b685b351c0ee90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/6.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede602592e274fa4bb654329044b4d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707bcc23c5c74ff99bdc5f2f39c26974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/994 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830ae0b1e44e4413b80022b38b278da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51572c914244c28a2e3d13252e63f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/144 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(100001, 768)\n",
       "    (wpe): Embedding(256, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=100001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dnagpt/gene_eng_gpt2_para_seg\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"dnagpt/gene_eng_gpt2_para_seg\")\n",
    "model.eval()  # 设置为评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f3e8f9-6589-4d0c-8d8d-2dfd4da007e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 使用微调后的模型对长文本进行段落分割\n",
    "def split_text_sliding_window(text, tokenizer, max_length=256, stride=64):\n",
    "    \"\"\"\n",
    "    使用滑动窗口将超长文本切分为多个段，每段长度不超过 max_length。\n",
    "\n",
    "    参数:\n",
    "        text (str): 超长输入文本。\n",
    "        tokenizer (PreTrainedTokenizer): 分词器。\n",
    "        max_length (int): 分段的最大长度。\n",
    "        stride (int): 滑动窗口的步长，控制段之间的重叠区域。\n",
    "\n",
    "    返回:\n",
    "        List[str]: 分割后的文本段。\n",
    "    \"\"\"\n",
    "    # 分词\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # 滑动窗口分段\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokenized_text):\n",
    "        end = min(start + max_length, len(tokenized_text))\n",
    "        chunks.append(tokenizer.decode(tokenized_text[start:end], skip_special_tokens=True))\n",
    "        start += max_length - stride  # 移动窗口，保留 overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def segment_text(text, top_k=5, max_length=256, stride=64):\n",
    "    \"\"\"\n",
    "    使用微调后的 GPT-2 模型对输入长文本进行段落分割，动态调整阈值。\n",
    "\n",
    "    参数:\n",
    "        text (str): 输入长文本。\n",
    "        model_path (str): 微调模型的路径。\n",
    "        top_k (int): 动态阈值计算时，选择预测分布中前 K 个概率的平均值。\n",
    "        max_length (int): 模型最大输入长度。\n",
    "        stride (int): 滑动窗口的步长。\n",
    "\n",
    "    返回:\n",
    "        str: 带有段落标记 <p_end> 的分割文本。\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    text = text.replace(\".\", \"\") #训练的时候去掉了.,防止干扰\n",
    "\n",
    "\n",
    "    # 分段文本\n",
    "    chunks = split_text_sliding_window(text, tokenizer, max_length, stride)\n",
    "\n",
    "    # 对每段文本进行推理\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs[\"input_ids\"])\n",
    "            logits = outputs.logits  # [batch_size, seq_length, vocab_size]\n",
    "\n",
    "        # 找到 <p_end> 的 token ID\n",
    "        p_end_id = tokenizer.convert_tokens_to_ids(\"<p_end>\")\n",
    "\n",
    "        # 计算 <p_end> 的概率分布\n",
    "        probabilities = torch.softmax(logits, dim=-1)  # 转为概率分布\n",
    "        p_end_probs = probabilities[0, :, p_end_id]  # [seq_length]\n",
    "\n",
    "        # 动态计算阈值：取前 top_k 概率的平均值\n",
    "        sorted_probs, _ = torch.sort(p_end_probs, descending=True)\n",
    "        threshold = sorted_probs[:top_k].mean().item()\n",
    "\n",
    "        # 根据动态阈值插入段落标记\n",
    "        tokens = inputs[\"input_ids\"][0].tolist()\n",
    "        segmented_tokens = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            segmented_tokens.append(token)\n",
    "            if p_end_probs[i] > threshold:\n",
    "                segmented_tokens.append(p_end_id)\n",
    "\n",
    "        # 解码为文本\n",
    "        segmented_text = tokenizer.decode(segmented_tokens, skip_special_tokens=False)\n",
    "        results.append(segmented_text)\n",
    "\n",
    "    # 合并结果\n",
    "    return \" \".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faab54cb-a198-4614-9d4b-417bc6c57bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Text:\n",
      "Mult iling ual transfer ability , which reflects how well models fine - tuned on one source language can be applied to other languages , has been well studied in mult iling ual pre - trained models However , the existence of such capability transfer between natural language and gene sequences / languages remains under explored This study addresses this gap by drawing inspiration from the sentence - pair classification task used for evaluating sentence similarity in natural language We constructed two analog ous tasks : DNA - pair classification ( DNA sequence similarity ) and DNA - protein - pair classification ( gene coding determination ) These tasks were designed to validate the transfer ability of capabilities from natural language to gene sequences Even a small - scale pre - trained model like GP T - 2 - small , which was pre - trained on English , achieved an accuracy of 78 % on the DNA - pair classification task after being fine - tuned on English sentence - pair classification data ( X TRE ME P <p_end> AWS - X <p_end> ) While training a BER T model on mult iling ual text , the precision reached 89 % On the more complex DNA - protein - pair classification task , however , the model ' s output was barely distinguish able from random output Exper imental validation has confirmed that the transfer of capabilities from natural language to biological language is unequivoc ally present Building on this foundation , we have also investigated the ual <p_end> text , the precision reached 89 % On <p_end> the more complex DNA - protein - pair classification task , however , the model ' s output was barely distinguish able from random output Exper imental validation has confirmed that the transfer of capabilities from natural language to biological language is unequivoc ally present Building on this foundation , we have also investigated the impact of model parameter scale and pre - training on this capability transfer We provide recommendations for facilit ating the transfer of capabilities from natural language to genetic language , as well as new approaches for conducting biological research based on this capability This study offers an intriguing new perspective on exploring the relationship between natural language and genetic language\n"
     ]
    }
   ],
   "source": [
    "# 示例输入长文本\n",
    "input_text = \"Multilingual transfer ability, which reflects how well models fine-tuned on one source language can be applied to other languages, has been well studied in multilingual pre-trained models. However, the existence of such capability transfer between natural language and gene sequences/languages remains under explored.This study addresses this gap by drawing inspiration from the sentence-pair classification task used for evaluating sentence similarity in natural language. We constructed two analogous tasks: DNA-pair classification(DNA sequence similarity) and DNA-protein-pair classification(gene coding determination). These tasks were designed to validate the transferability of capabilities from natural language to gene sequences. Even a small-scale pre-trained model like GPT-2-small, which was pre-trained on English, achieved an accuracy of 78% on the DNA-pair classification task after being fine-tuned on English sentence-pair classification data(XTREME PAWS-X).  While training a BERT model on multilingual text, the precision reached 89%. On the more complex DNA-protein-pair classification task, however, the model's output was barely distinguishable from random output.Experimental validation has confirmed that the transfer of capabilities from natural language to biological language is unequivocally present. Building on this foundation, we have also investigated the impact of model parameter scale and pre-training on this capability transfer. We provide recommendations for facilitating the transfer of capabilities from natural language to genetic language,as well as new approaches for conducting biological research based on this capability.This study offers an intriguing new perspective on exploring the relationship between natural language and genetic language.\"\n",
    "\n",
    "# 调用分段函数\n",
    "segmented_output = segment_text(input_text)\n",
    "print(\"Segmented Text:\")\n",
    "print(segmented_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
