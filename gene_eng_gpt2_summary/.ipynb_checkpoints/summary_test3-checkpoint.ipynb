{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fed6502-b63d-4c0f-b479-16a4a08b6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e99399-eb32-4ebc-8ceb-33ecf0e059a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ced023-fa39-4c26-a159-fb62789d4b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fadbf304bf4e17ae8fad0fbc4d5d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/997 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a957405f2e844f1fabe35045358ffe1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/6.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f309188d4514a7a892d6e951b23a39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/319 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87d4582f53b404288d77500c789bc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3071df93a5349abbb457dade2dced86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3dee9e35504c22b941efdbec4b9769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/144 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载 GPT-2 分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dnagpt/gene_eng_gpt2_summary\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 设置填充标记为 EOS 标记\n",
    "\n",
    "# 6. 加载 GPT-2 模型\n",
    "model = GPT2LMHeadModel.from_pretrained(\"dnagpt/gene_eng_gpt2_summary\")\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c0f30c-4da3-4b98-80b0-95b708a2942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sequence(sequence):\n",
    "    # 定义字符集（所有字符都假设为大写）\n",
    "    dna_chars = set('ACGT')\n",
    "    protein_chars = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "    english_chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ,.!?:;-\"\\'()')\n",
    "\n",
    "    # 去除空格并检查长度\n",
    "    sequence = sequence.strip()  # \n",
    "    \n",
    "    # 检查是否为DNA序列\n",
    "    if all(c in dna_chars for c in sequence):\n",
    "        return \"DNA\"\n",
    "    \n",
    "    # 检查是否为蛋白质序列\n",
    "    if all(c in protein_chars for c in sequence):\n",
    "        return \"Protein\"\n",
    "    \n",
    "    # 检查是否为英文文本（允许大小写字母、数字及常见标点符号）\n",
    "    if all(c in english_chars for c in sequence):\n",
    "        return \"English\"\n",
    "    \n",
    "    # 如果不符合上述任何条件，则无法明确分类\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fdb7866-0520-40c4-9875-5812879d676a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19413"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获得DNA和英文词表  只要长度2个及以上的词\n",
    "word_dict = tokenizer.get_vocab()\n",
    "\n",
    "DNA_token_list = []\n",
    "\n",
    "for word in word_dict:\n",
    "    word_type = classify_sequence(word)\n",
    "    if \"DNA\"==word_type:\n",
    "        DNA_token_list.append(word)\n",
    "\n",
    "len(DNA_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9afe92-1848-45c3-a597-a8393dc9d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessorList, LogitsProcessor\n",
    "import torch\n",
    "\n",
    "class DNAOnlyLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, allowed_tokens, tokenizer):\n",
    "        self.allowed_token_ids = tokenizer.convert_tokens_to_ids(allowed_tokens)\n",
    "    \n",
    "    def __call__(self, input_ids, scores):\n",
    "        # 创建掩码，将不允许的 token 的分数设为 -inf\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_token_ids] = 0\n",
    "        scores += mask\n",
    "        return scores\n",
    "\n",
    "def get_summary_with_constraints(text, DNA_token_list):\n",
    "    # 确保输入文本的预处理\n",
    "    text = text.strip() + \" TL;DR:\"\n",
    "    \n",
    "    # 对输入文本进行编码\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256,  # 输入文本的最大长度\n",
    "    )\n",
    "\n",
    "    # 创建 DNA 限制的 LogitsProcessor\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        DNAOnlyLogitsProcessor(DNA_token_list, tokenizer)\n",
    "    ])\n",
    "    \n",
    "    # 使用 max_new_tokens 控制生成长度\n",
    "    output = model.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"],\n",
    "        attention_mask=encoded_input[\"attention_mask\"],\n",
    "        max_new_tokens=16,       # 控制生成的新增文本长度\n",
    "        num_beams=5,             # 控制生成文本的多样性\n",
    "        logits_processor=logits_processor,\n",
    "        no_repeat_ngram_size=3,  # 避免生成重复内容\n",
    "        early_stopping=True,     # 提前终止生成\n",
    "    )\n",
    "    \n",
    "    # 对生成的输出进行解码\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 提取生成的摘要部分\n",
    "    summary = generated_text[len(text)+len(encoded_input[\"input_ids\"][0])-1:].strip() #字符长度+多出来的空格-1\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc600a88-95ce-4797-9a11-3cfc51abc17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG TATTGCC CCC TTGGC ATTTTTCC TATTTTG TTGCC TTACAACC TGGAATT AAAATGG ATTTTTT GGGGG TTTG TATCATT TGATTTAC ACAAC\n"
     ]
    }
   ],
   "source": [
    "# 示例用法\n",
    "#test_text = \"The DNA sequence analysis showed remarkable results.\"\n",
    "test_text = \"GTTATAACCTGTGAGAGTATGTTGGCGGTTTGTTGCACCTACCTTTCAAACCTCTTGTTCTTCCTGTGATTTATTTGAGGCACTCAAGTGGACAGAGACCATGAGAAATTTGAGTGGAGGCCATGTCGAAGAGTTTGTCTTGGTGGGTTTCCCTACCACTCCTCCCTTCCAGCTGCTCCTCTTTGTCCTTTTCTTTGCAATTTACCTTCTGACATTGTTGGAGAATGCACTCATTGTCTTCACAATATGGCTCACTCCAAGCCTTCATCGCCCCATGTACTTTTTCCTTGGCCATCTTTCTTTCCTGGAGCTTTGGTACATCAACGTCACCATTCCTCAGCTCTTGGCAGCCTTTCTTACCCAGGATAGTAGAGTCTCCTATGTAGGTTGCATGACCCAACTCTACTTCTTTATTGCCTTAGCCTGTACTGAATGTGTGCTGTTGGCAGTTATGGCCTATGACCGC\"\n",
    "\n",
    "print(get_summary_with_constraints(test_text, DNA_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea1553-5086-40e1-a094-4e1d527dbcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
