{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69109b9d-c5f3-4de2-a748-b2cb74f880f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960559d5-af43-4db8-ab35-6cf74d47461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f3e8f9-6589-4d0c-8d8d-2dfd4da007e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 使用微调后的模型对长文本进行段落分割\n",
    "def split_text_sliding_window(text, tokenizer, max_length=1024, stride=256):\n",
    "    \"\"\"\n",
    "    使用滑动窗口将超长文本切分为多个段，每段长度不超过 max_length。\n",
    "\n",
    "    参数:\n",
    "        text (str): 超长输入文本。\n",
    "        tokenizer (PreTrainedTokenizer): 分词器。\n",
    "        max_length (int): 分段的最大长度。\n",
    "        stride (int): 滑动窗口的步长，控制段之间的重叠区域。\n",
    "\n",
    "    返回:\n",
    "        List[str]: 分割后的文本段。\n",
    "    \"\"\"\n",
    "    # 分词\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # 滑动窗口分段\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokenized_text):\n",
    "        end = min(start + max_length, len(tokenized_text))\n",
    "        chunks.append(tokenizer.decode(tokenized_text[start:end], skip_special_tokens=True))\n",
    "        start += max_length - stride  # 移动窗口，保留 overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def segment_text(text, model_path=\"gpt2\", top_k=15, max_length=1024, stride=256):\n",
    "    \"\"\"\n",
    "    使用微调后的 GPT-2 模型对输入长文本进行段落分割，动态调整阈值。\n",
    "\n",
    "    参数:\n",
    "        text (str): 输入长文本。\n",
    "        model_path (str): 微调模型的路径。\n",
    "        top_k (int): 动态阈值计算时，选择预测分布中前 K 个概率的平均值。\n",
    "        max_length (int): 模型最大输入长度。\n",
    "        stride (int): 滑动窗口的步长。\n",
    "\n",
    "    返回:\n",
    "        str: 带有段落标记 <p_end> 的分割文本。\n",
    "    \"\"\"\n",
    "    import torch\n",
    "\n",
    "    # 加载模型和分词器\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    model.eval()  # 设置为评估模式\n",
    "\n",
    "    # 分段文本\n",
    "    chunks = split_text_sliding_window(text, tokenizer, max_length, stride)\n",
    "\n",
    "    # 对每段文本进行推理\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs[\"input_ids\"])\n",
    "            logits = outputs.logits  # [batch_size, seq_length, vocab_size]\n",
    "\n",
    "        # 找到 <p_end> 的 token ID\n",
    "        p_end_id = tokenizer.convert_tokens_to_ids(\".\")\n",
    "\n",
    "        # 计算 <p_end> 的概率分布\n",
    "        probabilities = torch.softmax(logits, dim=-1)  # 转为概率分布\n",
    "        p_end_probs = probabilities[0, :, p_end_id]  # [seq_length]\n",
    "\n",
    "        # 动态计算阈值：取前 top_k 概率的平均值\n",
    "        sorted_probs, _ = torch.sort(p_end_probs, descending=True)\n",
    "        threshold = sorted_probs[:top_k].mean().item()\n",
    "\n",
    "        # 根据动态阈值插入段落标记\n",
    "        tokens = inputs[\"input_ids\"][0].tolist()\n",
    "        segmented_tokens = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            segmented_tokens.append(token)\n",
    "            if p_end_probs[i] > threshold:\n",
    "                segmented_tokens.append(p_end_id)\n",
    "\n",
    "        # 解码为文本\n",
    "        segmented_text = tokenizer.decode(segmented_tokens, skip_special_tokens=False)\n",
    "        results.append(segmented_text)\n",
    "\n",
    "    # 合并结果\n",
    "    return \" \".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faab54cb-a198-4614-9d4b-417bc6c57bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual transfer ability, which reflects how well models fine-tuned on one source language can be applied to other languages, has been well studied in multilingual pre-trained models. However, the existence of such capability transfer between natural language and gene sequences/languages remains under explored.This study addresses this gap by drawing inspiration from the sentence-pair classification task used for evaluating sentence similarity in natural language. We constructed two analogous tasks: DNA-pair classification(DNA sequence similarity) and DNA-protein-pair classification(gene coding determination). These tasks were designed to validate the transferability of capabilities from natural language to gene sequences. Even a small-scale pre-trained model like GPT-2-small, which was pre-trained on English, achieved an accuracy of 78% on the DNA-pair classification task after being fine-tuned on English sentence-pair classification data(XTREME PAWS-X).  While training a BERT model on multilingual text, the precision reached 89%. On the more complex DNA-protein-pair classification task, however, the model's output was barely distinguishable from random output.Experimental validation has confirmed that the transfer of capabilities from natural language to biological language is unequivocally present. Building on this foundation, we have also investigated the impact of model parameter scale and pre-training on this capability transfer. We provide recommendations for facilitating the transfer of capabilities from natural language to genetic language,as well as new approaches for conducting biological research based on this capability.This study offers an intriguing new perspective on exploring the relationship between natural language and genetic language.\n",
      "Multilingual transfer ability, which reflects how well models fine-tuned on one source language can be applied to other languages., has been well studied in multilingual pre-trained models. However, the existence of such capability transfer between natural language and gene sequences/languages remains under explored.This study addresses this gap by drawing inspiration from the sentence-pair classification task used for evaluating sentence similarity in natural language We constructed two analogous tasks: DNA-pair classification(DNA sequence similarity) and DNA-protein-pair classification(gene coding determination) These tasks were designed to validate the transferability of capabilities from natural language to gene sequences Even a small-scale pre-trained model like GPT-2-small, which was pre-trained on English, achieved an accuracy of 78% on the DNA-pair classification task. after being fine-tuned on English. sentence-pair classification data.(XTREME PAWS-X)  While training a BERT model on multilingual text, the precision reached 89% On the more complex DNA-protein-pair classification task, however, the model's output was barely distinguishable from random outputExperimental validation has confirmed that the transfer of capabilities from natural language to biological language is unequivocally present Building on this foundation, we have also investigated the impact of model parameter scale and pre-training on this capability transfer We provide recommendations for facilitating the transfer of capabilities from natural language to genetic language,as well as new approaches for conducting biological research based on this capabilityThis study offers an intriguing new perspective on exploring the relationship between natural language and genetic language\n"
     ]
    }
   ],
   "source": [
    "# 示例输入长文本\n",
    "input_text = \"Multilingual transfer ability, which reflects how well models fine-tuned on one source language can be applied to other languages, has been well studied in multilingual pre-trained models. However, the existence of such capability transfer between natural language and gene sequences/languages remains under explored.This study addresses this gap by drawing inspiration from the sentence-pair classification task used for evaluating sentence similarity in natural language. We constructed two analogous tasks: DNA-pair classification(DNA sequence similarity) and DNA-protein-pair classification(gene coding determination). These tasks were designed to validate the transferability of capabilities from natural language to gene sequences. Even a small-scale pre-trained model like GPT-2-small, which was pre-trained on English, achieved an accuracy of 78% on the DNA-pair classification task after being fine-tuned on English sentence-pair classification data(XTREME PAWS-X).  While training a BERT model on multilingual text, the precision reached 89%. On the more complex DNA-protein-pair classification task, however, the model's output was barely distinguishable from random output.Experimental validation has confirmed that the transfer of capabilities from natural language to biological language is unequivocally present. Building on this foundation, we have also investigated the impact of model parameter scale and pre-training on this capability transfer. We provide recommendations for facilitating the transfer of capabilities from natural language to genetic language,as well as new approaches for conducting biological research based on this capability.This study offers an intriguing new perspective on exploring the relationship between natural language and genetic language.\"\n",
    "input_text_noseq = input_text.replace(\".\",\"\")\n",
    "\n",
    "# 调用分段函数\n",
    "segmented_output = segment_text(input_text_noseq)\n",
    "print(input_text)\n",
    "print(segmented_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76704a2f-0845-41a8-8887-1f571b92768c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
