{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5dcfe41-d48d-4e5a-929d-76446da83b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbac3ce-9b63-4129-a8f4-d2a445868d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "#分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dnagpt/gene_eng_gpt2_v1_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b6bfd2-be51-4fb1-a81e-dccd1f354194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = tokenizer.get_vocab()\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bea266-644a-478b-bf83-6f34079c8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model,AutoModel\n",
    "import torch\n",
    "model_name=\"dnagpt/gene_eng_gpt2_v1_ft\"\n",
    "device=\"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"\n",
    "    使用 GPT-2 模型获取文本的向量表示。\n",
    "    \n",
    "    参数:\n",
    "        text (str): 输入文本。\n",
    "        model_name (str): 预训练 GPT-2 模型名称，默认为 \"gpt2\"。\n",
    "        device (str): 设备名称（\"cpu\" 或 \"cuda\"）。\n",
    "    \n",
    "    返回:\n",
    "        torch.Tensor: 文本的向量表示，维度为 [hidden_size]。\n",
    "    \"\"\"\n",
    "\n",
    "    # 将文本编码为输入 ID 并添加批量维度\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # 获取模型的隐藏层输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [batch_size, seq_length, hidden_size]\n",
    "    \n",
    "    # 平均池化：获取序列中所有词向量的平均值\n",
    "    embeddings = hidden_states.mean(dim=1).squeeze()  # [hidden_size]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c38a694-078c-44f3-99cd-07af734a951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AGCT' is classified as: DNA\n",
      "'MVLFRSSGYV' is classified as: Protein\n",
      "'HELLO WORLD' is classified as: English\n",
      "'AGCZ' is classified as: English\n",
      "'XYZ' is classified as: English\n",
      "'A T G C' is classified as: English\n",
      "'HELLO, WORLD!' is classified as: English\n",
      "'ABC' is classified as: English\n"
     ]
    }
   ],
   "source": [
    "def classify_sequence(sequence):\n",
    "    # 定义字符集（所有字符都假设为大写）\n",
    "    dna_chars = set('ACGT')\n",
    "    protein_chars = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "    english_chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ,.!?:;-\"\\'()')\n",
    "\n",
    "    # 去除空格并检查长度\n",
    "    sequence = sequence.strip()  # \n",
    "    \n",
    "    # 检查是否为DNA序列\n",
    "    if all(c in dna_chars for c in sequence):\n",
    "        return \"DNA\"\n",
    "    \n",
    "    # 检查是否为蛋白质序列\n",
    "    if all(c in protein_chars for c in sequence):\n",
    "        return \"Protein\"\n",
    "    \n",
    "    # 检查是否为英文文本（允许大小写字母、数字及常见标点符号）\n",
    "    if all(c in english_chars for c in sequence):\n",
    "        return \"English\"\n",
    "    \n",
    "    # 如果不符合上述任何条件，则无法明确分类\n",
    "    return \"Unknown\"\n",
    "\n",
    "# 示例用法\n",
    "sequences = [\"AGCT\", \"MVLFRSSGYV\", \"HELLO WORLD\", \"AGCZ\", \"XYZ\", \"A T G C\", \"HELLO, WORLD!\", \"ABC\"]\n",
    "for seq in sequences:\n",
    "    print(f\"'{seq}' is classified as: {classify_sequence(seq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f451d4-d79a-45f1-a76e-90da7b160d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19409 24193 40124\n"
     ]
    }
   ],
   "source": [
    "#获得DNA和英文词表  只要长度2个及以上的词\n",
    "dna_word_list = []\n",
    "eng_word_list = []\n",
    "protein_word_list = []\n",
    "\n",
    "for word in word_dict:\n",
    "    if len(word)>=2:\n",
    "        word_type = classify_sequence(word)\n",
    "        if \"DNA\"==word_type:\n",
    "            dna_word_list.append(word)\n",
    "\n",
    "        if \"Protein\"==word_type:\n",
    "            protein_word_list.append(word)\n",
    "\n",
    "        if \"English\"==word_type:\n",
    "            eng_word_list.append(word)\n",
    "\n",
    "        \n",
    "\n",
    "print(len(dna_word_list), len(eng_word_list), len(protein_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d38d0b-b0b5-45fb-b923-09d848d3af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_word_vect_dict = {}\n",
    "eng_word_vect_dict = {}\n",
    "for word in dna_word_list:\n",
    "    word_vect = get_text_embedding(word)\n",
    "    dna_word_vect_dict[word] = word_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e404735d-777c-4cdb-a3c3-55651e940920",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in eng_word_list:\n",
    "    word_vect = get_text_embedding(word)\n",
    "    eng_word_vect_dict[word] = word_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3f5fed8-dd22-4991-932a-8868e24dc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def find_most_similar_optimized(dna_word_vect_dict, eng_word_vect_dict):\n",
    "    \"\"\"\n",
    "    使用 KD-Tree 加速 DNA 单词到英文单词的匹配。\n",
    "    \n",
    "    参数:\n",
    "        dna_word_vect_dict (dict): DNA 单词与其向量的字典 {dna_word: dna_vector}.\n",
    "        eng_word_vect_dict (dict): 英文单词与其向量的字典 {eng_word: eng_vector}.\n",
    "    \n",
    "    返回:\n",
    "        dict: DNA 单词到英文单词的映射词典 {dna_word: most_similar_eng_word}.\n",
    "    \"\"\"\n",
    "    # 构建英文单词向量矩阵和对应单词列表\n",
    "    eng_words = list(eng_word_vect_dict.keys())\n",
    "    \n",
    "    # 确保向量在 CPU 上并转换为 NumPy 数组\n",
    "    eng_vectors = np.array([v.cpu().numpy() if isinstance(v, torch.Tensor) else v for v in eng_word_vect_dict.values()])\n",
    "    \n",
    "    # 初始化最近邻搜索模型\n",
    "    nn_model = NearestNeighbors(metric=\"cosine\").fit(eng_vectors)\n",
    "    \n",
    "    dna_eng_dict = {}\n",
    "    \n",
    "    for dna_word, dna_vector in dna_word_vect_dict.items():\n",
    "        # 将 DNA 向量确保在 CPU 并转换为 NumPy 数组\n",
    "        if isinstance(dna_vector, torch.Tensor):\n",
    "            dna_vector = dna_vector.cpu().numpy()\n",
    "        \n",
    "        # 查找最近的英文单词\n",
    "        distances, indices = nn_model.kneighbors([dna_vector], n_neighbors=1)\n",
    "        most_similar_eng_word = eng_words[indices[0][0]]\n",
    "        \n",
    "        # 记录匹配结果\n",
    "        dna_eng_dict[dna_word] = most_similar_eng_word\n",
    "    \n",
    "    return dna_eng_dict\n",
    "\n",
    "# 示例调用\n",
    "dna_eng_dict_optimized = find_most_similar_optimized(dna_word_vect_dict, eng_word_vect_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ace06a-d14a-440a-be5d-1738b48eb0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92fb0b32-1b6a-4f6b-9130-073a3a7a515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA-English dictionary has been saved to dna_eng_dict_optimized.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 将 dna_eng_dict_optimized 保存到 JSON 文件中\n",
    "def save_dict_to_json(data_dict, file_path):\n",
    "    \"\"\"\n",
    "    将字典保存为 JSON 文件。\n",
    "    \n",
    "    参数:\n",
    "        data_dict (dict): 要保存的字典。\n",
    "        file_path (str): 保存 JSON 文件的路径。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 示例调用\n",
    "save_dict_to_json(dna_eng_dict_optimized, \"dna_eng_dict_optimized.json\")\n",
    "print(\"DNA-English dictionary has been saved to dna_eng_dict_optimized.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831dd86e-2765-4db2-9fa8-396cc9ad1f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'olia': 5117,\n",
       " 'umbai': 2040,\n",
       " 'stic': 27,\n",
       " 'peninsula': 2966,\n",
       " 'iciency': 22,\n",
       " 'eleph': 73,\n",
       " 'pson': 446,\n",
       " 'ala': 589,\n",
       " 'politan': 2219,\n",
       " 'https': 2,\n",
       " 'transported': 1883,\n",
       " 'icking': 1249,\n",
       " 'displaystyle': 53,\n",
       " 'cemet': 10,\n",
       " 'icipal': 1138,\n",
       " 'coln': 54,\n",
       " 'idence': 108,\n",
       " 'atherine': 47,\n",
       " 'olph': 108,\n",
       " 'beha': 39,\n",
       " 'desirable': 121,\n",
       " 'atting': 26,\n",
       " 'inflamm': 14,\n",
       " 'surroundings': 85,\n",
       " 'mamm': 221,\n",
       " 'demean': 5,\n",
       " 'hower': 52,\n",
       " 'annah': 19,\n",
       " 'ushima': 54,\n",
       " 'oples': 3,\n",
       " 'enty': 30,\n",
       " 'directions': 1,\n",
       " 'apore': 21,\n",
       " 'duc': 31,\n",
       " 'XXXXXXXX': 2,\n",
       " 'unsupported': 1,\n",
       " 'electro': 21,\n",
       " 'ashed': 46,\n",
       " 'T1': 4,\n",
       " 'ometimes': 54,\n",
       " 'ancing': 1,\n",
       " 'mechanic': 5,\n",
       " 'atican': 16,\n",
       " 'entirety': 6,\n",
       " 'archite': 2,\n",
       " 'employs': 12,\n",
       " 'Resour': 1,\n",
       " 'enjoyable': 2,\n",
       " 'ving': 3,\n",
       " 'rance': 11,\n",
       " 'northwest': 8,\n",
       " 'ampions': 13,\n",
       " 'XXXXXXXXXXXX': 5,\n",
       " 'Weap': 5,\n",
       " 'XT': 7,\n",
       " 'amen': 21,\n",
       " 'Duter': 4,\n",
       " 'ampion': 1,\n",
       " 'agonal': 1,\n",
       " 'involve': 4,\n",
       " 'underneath': 2,\n",
       " 'rought': 19,\n",
       " 'Carneg': 11,\n",
       " 'antibi': 4,\n",
       " 'inery': 13,\n",
       " 'tural': 7,\n",
       " 'perspect': 1,\n",
       " 'grey': 7,\n",
       " 'necessarily': 8,\n",
       " 'iencies': 1,\n",
       " 'Cinc': 3,\n",
       " 'amy': 1,\n",
       " 'Performan': 1,\n",
       " 'itic': 2,\n",
       " '003': 2,\n",
       " 'pecu': 1,\n",
       " 'ambers': 1,\n",
       " 'berra': 8,\n",
       " 'incar': 1,\n",
       " 'itchen': 3,\n",
       " 'BS': 2,\n",
       " 'passeng': 2,\n",
       " 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX': 2,\n",
       " 'phosp': 4,\n",
       " 'ernand': 3,\n",
       " 'territ': 2,\n",
       " 'inity': 7,\n",
       " 'promin': 2,\n",
       " 'Sul': 1,\n",
       " 'minster': 1,\n",
       " 'ctica': 1,\n",
       " 'thern': 3,\n",
       " 'minerals': 1,\n",
       " 'solely': 1,\n",
       " 'hydr': 1,\n",
       " 'ribut': 2,\n",
       " 'knocked': 5,\n",
       " 'auded': 1,\n",
       " 'illi': 2,\n",
       " 'ichever': 2,\n",
       " 'performan': 6,\n",
       " 'A1': 2,\n",
       " 'iance': 1,\n",
       " 'Milwau': 2,\n",
       " 'Feat': 1,\n",
       " 'accompan': 1,\n",
       " 'atche': 3,\n",
       " 'href': 1,\n",
       " 'Ole': 2,\n",
       " 'gomery': 6,\n",
       " 'rhyth': 2,\n",
       " 'mouth': 3,\n",
       " 'nomine': 4,\n",
       " 'Jup': 1,\n",
       " 'mouths': 1,\n",
       " 'Mediter': 1,\n",
       " 'etsk': 1,\n",
       " 'icular': 3,\n",
       " 'gur': 1,\n",
       " 'dale': 1,\n",
       " 'XG': 1,\n",
       " 'BLO': 1,\n",
       " 'Dire': 1,\n",
       " 'predecess': 1,\n",
       " 'reliant': 3,\n",
       " 'izoph': 1,\n",
       " 'hefty': 3,\n",
       " 'reper': 3,\n",
       " 'coron': 1,\n",
       " 'occas': 1,\n",
       " 'itating': 1,\n",
       " 'vertisement': 1,\n",
       " 'depos': 1,\n",
       " 'oldown': 1,\n",
       " 'otro': 1,\n",
       " 'Pere': 2,\n",
       " 'Chi': 1,\n",
       " 'portray': 1,\n",
       " 'Damasc': 2,\n",
       " 'portrayed': 1,\n",
       " 'neurological': 1,\n",
       " 'Shakespe': 1,\n",
       " 'vironments': 1,\n",
       " 'IoT': 1,\n",
       " 'solute': 1,\n",
       " 'prefers': 1,\n",
       " 'dinosa': 1,\n",
       " 'thanol': 1,\n",
       " 'respon': 1,\n",
       " 'RX': 1,\n",
       " 'adays': 4,\n",
       " 'Inspe': 2,\n",
       " 'manner': 1,\n",
       " 'subjected': 4,\n",
       " 'cription': 1,\n",
       " 'inosa': 3,\n",
       " 'whereas': 1,\n",
       " 'Myan': 1,\n",
       " 'headaches': 1,\n",
       " 'admire': 1,\n",
       " 'landsc': 2,\n",
       " 'icul': 1,\n",
       " 'membr': 1,\n",
       " 'entrepre': 1,\n",
       " 'contracep': 1,\n",
       " 'OE': 1,\n",
       " 'responsib': 1,\n",
       " 'XXXXXXXXXXXXXXXX': 1,\n",
       " 'certific': 1,\n",
       " 'ception': 1,\n",
       " 'inflammatory': 1,\n",
       " 'inh': 1,\n",
       " 'reactive': 1,\n",
       " 'arring': 1,\n",
       " 'ral': 1,\n",
       " 'enormous': 1,\n",
       " 'behavi': 1,\n",
       " 'frontal': 2,\n",
       " 'impe': 3,\n",
       " 'oliber': 1,\n",
       " 'charac': 1,\n",
       " 'helicop': 1,\n",
       " 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX': 1,\n",
       " 'malink': 1,\n",
       " 'thumb': 1,\n",
       " 'igue': 1,\n",
       " 'influencing': 1,\n",
       " 'enna': 1,\n",
       " 'discre': 1,\n",
       " 'Pover': 1,\n",
       " 'teous': 1,\n",
       " 'toos': 1,\n",
       " 'URE': 1,\n",
       " 'DX': 1,\n",
       " 'trig': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_word_dict = {}\n",
    "for dna_word in dna_eng_dict_optimized:\n",
    "    en_word = dna_eng_dict_optimized[dna_word]\n",
    "    en_word_dict.setdefault(en_word,0)\n",
    "    en_word_dict[en_word] = en_word_dict[en_word] + 1\n",
    "\n",
    "en_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c36edad3-e33e-4fd7-a461-84c7c995c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def find_most_similar_with_randomization(dna_word_vect_dict, eng_word_vect_dict, top_k=500):\n",
    "    \"\"\"\n",
    "    使用 KD-Tree 加速 DNA 单词到英文单词的匹配，并随机选择最近的 top_k 单词中的一个作为映射。\n",
    "\n",
    "    参数:\n",
    "        dna_word_vect_dict (dict): DNA 单词与其向量的字典 {dna_word: dna_vector}.\n",
    "        eng_word_vect_dict (dict): 英文单词与其向量的字典 {eng_word: eng_vector}.\n",
    "        top_k (int): 随机选择时从最近的 top_k 单词中选取。\n",
    "\n",
    "    返回:\n",
    "        dict: DNA 单词到英文单词的映射词典 {dna_word: random_eng_word_from_top_k}.\n",
    "    \"\"\"\n",
    "    # 构建英文单词向量矩阵和对应单词列表\n",
    "    eng_words = list(eng_word_vect_dict.keys())\n",
    "    \n",
    "    # 确保向量在 CPU 上并转换为 NumPy 数组\n",
    "    eng_vectors = np.array([v.cpu().numpy() if isinstance(v, torch.Tensor) else v for v in eng_word_vect_dict.values()])\n",
    "\n",
    "    # 初始化最近邻搜索模型\n",
    "    nn_model = NearestNeighbors(metric=\"cosine\").fit(eng_vectors)\n",
    "\n",
    "    dna_eng_dict = {}\n",
    "\n",
    "    for dna_word, dna_vector in dna_word_vect_dict.items():\n",
    "        # 将 DNA 向量确保在 CPU 并转换为 NumPy 数组\n",
    "        if isinstance(dna_vector, torch.Tensor):\n",
    "            dna_vector = dna_vector.cpu().numpy()\n",
    "\n",
    "        # 查找最近的 top_k 英文单词\n",
    "        distances, indices = nn_model.kneighbors([dna_vector], n_neighbors=top_k)\n",
    "        top_k_eng_words = [eng_words[idx] for idx in indices[0]]\n",
    "\n",
    "        # 随机选择一个单词\n",
    "        random_eng_word = random.choice(top_k_eng_words)\n",
    "\n",
    "        # 记录匹配结果\n",
    "        dna_eng_dict[dna_word] = random_eng_word\n",
    "\n",
    "    return dna_eng_dict\n",
    "\n",
    "# 示例调用\n",
    "dna_eng_dict_randomized = find_most_similar_with_randomization(dna_word_vect_dict, eng_word_vect_dict, top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e68e6e6-3c91-402e-8cac-04d595e64c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_word_dict = {}\n",
    "for dna_word in dna_eng_dict_randomized:\n",
    "    en_word = dna_eng_dict_randomized[dna_word]\n",
    "    en_word_dict.setdefault(en_word,0)\n",
    "    en_word_dict[en_word] = en_word_dict[en_word] + 1\n",
    "\n",
    "len(en_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "490818e7-f635-4897-94ab-511dfd9ea78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_suffix_to_dict(dna_eng_dict):\n",
    "    \"\"\"\n",
    "    为 DNA 到英文单词的映射词典添加唯一后缀，防止多个 DNA 单词对应同一个英文单词。\n",
    "\n",
    "    参数:\n",
    "        dna_eng_dict (dict): {dna_word: eng_word} 形式的映射词典。\n",
    "\n",
    "    返回:\n",
    "        dict: 添加后缀后的映射词典。\n",
    "    \"\"\"\n",
    "    # 统计每个英文单词的映射次数\n",
    "    eng_word_count = {}\n",
    "    for dna_word, eng_word in dna_eng_dict.items():\n",
    "        if eng_word not in eng_word_count:\n",
    "            eng_word_count[eng_word] = 0\n",
    "        eng_word_count[eng_word] += 1\n",
    "\n",
    "    # 为映射次数超过 1 的英文单词添加后缀\n",
    "    eng_word_suffix_count = {key: 1 for key in eng_word_count.keys()}\n",
    "    updated_dict = {}\n",
    "    for dna_word, eng_word in dna_eng_dict.items():\n",
    "        if eng_word_count[eng_word] > 1:\n",
    "            # 添加后缀\n",
    "            unique_eng_word = f\"{eng_word}{eng_word_suffix_count[eng_word]}\"\n",
    "            eng_word_suffix_count[eng_word] += 1\n",
    "        else:\n",
    "            unique_eng_word = eng_word\n",
    "        updated_dict[dna_word] = unique_eng_word\n",
    "\n",
    "    return updated_dict\n",
    "\n",
    "# 示例调用\n",
    "dna_eng_dict_unique = add_unique_suffix_to_dict(dna_eng_dict_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be92a8a7-37c8-4b03-b400-026308d20a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 保存 dna_eng_dict_unique 到 JSON 文件\n",
    "output_file = \"dna_eng_dict_unique.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(dna_eng_dict_unique, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3987d-8c1f-46cd-b954-e3ec0ada5faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
